{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c1b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d6f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "LOCAL_NOTEBOOK= True  #Local or in Kaggle\n",
    "if LOCAL_NOTEBOOK:\n",
    "\n",
    "    DEVICE = 'gpu'\n",
    "\n",
    "    save_models_path = r'..\\data\\kaggle_playground\\calories_competition\\models'\n",
    "    kaggle_path = glob.glob(r'..\\data\\kaggle_playground\\calories_competition\\*.csv')\n",
    "\n",
    "    csv_files = {\n",
    "        path.split(\"\\\\\")[-1][:-4]: path\n",
    "        for path in kaggle_path\n",
    "    }\n",
    "\n",
    "    df_test = pd.read_csv(csv_files['test'])\n",
    "    df_train= pd.read_csv(csv_files['train']).drop(columns=['id'])\n",
    "    df_train['Calories'] = df_train['Calories'].astype(int)\n",
    "    df_subsample = pd.read_csv(csv_files['sample_submission'])\n",
    "\n",
    "else:\n",
    "    # Kaggle read csvs\n",
    "\n",
    "    DEVICE = 'cpu'\n",
    "    df_test = pd.read_csv(r'/kaggle/input/playground-series-s5e5/test.csv')\n",
    "    df_train = pd.read_csv(r'/kaggle/input/playground-series-s5e5/train.csv')\n",
    "    df_subsample = pd.read_csv(r'/kaggle/input/playground-series-s5e5/train.csv')\n",
    "    save_models_path = r'/kaggle/working/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b016ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to(col: pd.Series):\n",
    "    decode = {}\n",
    "    encode = {}\n",
    "    \n",
    "    categorical_data = col.sort_index().unique()\n",
    "\n",
    "    for item in enumerate(categorical_data):\n",
    "\n",
    "        item_enc = {item[1]: item[0]}\n",
    "        item_dec = {item[0]: item[1]}\n",
    "        encode.update(item_enc)\n",
    "        decode.update(item_dec)\n",
    "    \n",
    "    return encode,decode\n",
    "        \n",
    "enc_sex, dec_sex = categorical_to(df_train['Sex'])\n",
    "\n",
    "df_train['Sex'] = df_train['Sex'].map(enc_sex)\n",
    "df_test['Sex'] = df_test['Sex'].map(enc_sex)\n",
    "\n",
    "features = list(df_train.drop(columns=['Calories']).columns)\n",
    "features_scaler = list(df_train.drop(columns=['Sex','Calories','Age']).columns)\n",
    "target = 'Calories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4afa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df_train[features].copy()\n",
    "y = df_train[target].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[features_scaler] = scaler.fit_transform(X[features_scaler])\n",
    "df_test[features_scaler] = scaler.transform(df_test[features_scaler])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb474f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 15:41:20,620] A new study created in memory with name: no-name-75b27503-5fc0-48ae-a021-8c63437932d2\n",
      "[I 2025-05-07 15:49:04,877] Trial 0 finished with value: -3.7182881639684235 and parameters: {'booster': 'dart', 'lambda': 0.0001862691402022658, 'alpha': 1.1175827499974151e-07, 'learning_rate': 0.23009108748345408, 'n_estimators': 900, 'max_depth': 6, 'subsample': 0.7746703833134805, 'colsample_bytree': 0.5793668594207906}. Best is trial 0 with value: -3.7182881639684235.\n",
      "[I 2025-05-07 15:49:19,683] Trial 1 finished with value: -3.6675830784255328 and parameters: {'booster': 'gbtree', 'lambda': 0.19023495851242614, 'alpha': 9.378756721550792e-05, 'learning_rate': 0.07395891534391195, 'n_estimators': 813, 'max_depth': 8, 'subsample': 0.5027248181268938, 'colsample_bytree': 0.5343832872687366}. Best is trial 1 with value: -3.6675830784255328.\n",
      "[I 2025-05-07 15:49:45,301] Trial 2 finished with value: -11.102693126591006 and parameters: {'booster': 'gblinear', 'lambda': 7.466805535951342e-08, 'alpha': 3.690299277104217e-07, 'learning_rate': 0.2841769047139114, 'n_estimators': 825}. Best is trial 1 with value: -3.6675830784255328.\n",
      "[I 2025-05-07 15:57:43,724] Trial 3 finished with value: -3.9076106735994123 and parameters: {'booster': 'dart', 'lambda': 3.6260300060053205e-08, 'alpha': 0.0008212802786660959, 'learning_rate': 0.2553382006405939, 'n_estimators': 891, 'max_depth': 8, 'subsample': 0.7111890523039645, 'colsample_bytree': 0.8455269024196215}. Best is trial 1 with value: -3.6675830784255328.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n",
    "\n",
    "    params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",  \n",
    "        \"eval_metric\": \"rmsle\",  \n",
    "        \"booster\": booster,\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 700, 900),\n",
    "        \"device\":\"gpu\"\n",
    "    }\n",
    "\n",
    "    if booster in [\"gbtree\", \"dart\"]:\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        params[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "    try:\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_root_mean_squared_error\").mean()\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(\"Failed with params:\", params)\n",
    "        raise e\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83c54a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree',\n",
       " 'lambda': 0.19023495851242614,\n",
       " 'alpha': 9.378756721550792e-05,\n",
       " 'learning_rate': 0.07395891534391195,\n",
       " 'n_estimators': 813,\n",
       " 'max_depth': 8,\n",
       " 'subsample': 0.5027248181268938,\n",
       " 'colsample_bytree': 0.5343832872687366}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6fc5a",
   "metadata": {},
   "source": [
    "### GradBoosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1fb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosters = [\"gbtree\", \"gblinear\"]\n",
    "\n",
    "models_name = ['xgb']\n",
    "\n",
    "\n",
    "if TRAIN_MODE:\n",
    "#GridSearchCV()\n",
    "\n",
    "    if 'xgb' in models_name:\n",
    "        model_xgb = xgb.XGBRegressor(\n",
    "            booster= 'gbtree',\n",
    "            reg_lambda= 0.19023495851242614,\n",
    "            alpha = 9.378756721550792e-05,\n",
    "            learning_rate = 0.07395891534391195,\n",
    "            n_estimators= 5000,\n",
    "            max_depth = 8,\n",
    "            subsample= 0.5027248181268938,\n",
    "            colsample_bytree= 0.5343832872687366,\n",
    "            device = DEVICE\n",
    "            , early_stopping_rounds=100\n",
    "        )\n",
    "\n",
    "\n",
    "        model_xgb.fit(X_train ,y_train ,eval_set=[(X_val,y_val)] ,verbose=False)\n",
    "\n",
    "    if 'cat' in models_name:\n",
    "        eval_data = cat.Pool(X_val ,y_val)\n",
    "        model_cat = cat.CatBoostRegressor(iterations= 10000 \n",
    "                                    ,task_type = DEVICE.upper()\t\n",
    "                                    ,learning_rate=0.001\n",
    "                                    ,depth=5\n",
    "                                    ,l2_leaf_reg= 2\n",
    "                                    ,verbose=0 \n",
    "                                    ,loss_function='RMSE')\n",
    "        \n",
    "        model_cat.fit(X_train, y_train, eval_set=[eval_data], early_stopping_rounds=100)\n",
    "        \n",
    "\n",
    "    if 'lgbm' in models_name:\n",
    "        model_lgbm = lgb.LGBMRegressor(n_estimators = 10000\n",
    "                                ,learning_rate = 0.001\n",
    "                                ,objective='regression'\n",
    "                                ,max_depth=5\n",
    "                                ,early_stopping_rounds=50\n",
    "                                ,metric='rmse'\n",
    "                                ,verbose=-1\n",
    "                                ,device='gpu')\n",
    "        \n",
    "        model_lgbm.fit(X_train, y_train , eval_set=[(X_val, y_val)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1123ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost: 0.06598023579961027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LynWin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [09:06:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'xgb' in models_name:\n",
    "\n",
    "    xgb_pred = model_xgb.predict(X_val)\n",
    "    xgb_pred = np.where(xgb_pred < 0, 0, xgb_pred)\n",
    "    xgb_loss = root_mean_squared_log_error(y_val,xgb_pred)\n",
    "    print(f\"Xgboost: {xgb_loss}\")\n",
    "\n",
    "if 'cat' in models_name:\n",
    "\n",
    "    cat_pred = model_cat.predict(X_val)\n",
    "    cat_pred = np.where(cat_pred < 0, 0, cat_pred)\n",
    "    cat_loss = root_mean_squared_log_error(y_val,cat_pred)\n",
    "    print(f\"Catboost: {cat_loss}\")\n",
    "\n",
    "if 'lgbm' in models_name:\n",
    "\n",
    "    lgbm_pred = model_lgbm.predict(X_val)\n",
    "    lgbm_pred = np.where(lgbm_pred < 0, 0, lgbm_pred)\n",
    "    lgbm_loss = root_mean_squared_log_error(y_val,lgbm_pred)\n",
    "    print(f\"LGBM: {lgbm_loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee3679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\kaggle_playground\\\\calories_competition\\\\models\\\\xgb_4.model']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'xgb'\n",
    "joblib.dump(model_xgb,f'{save_models_path}\\\\{model_name}_{4}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a9754be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE: 0.07\n"
     ]
    }
   ],
   "source": [
    "xgb_sub_pred= model_xgb.predict(df_test[features])\n",
    "y_pred_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "print(f\"Validation RMSLE: {round(root_mean_squared_log_error(y_val, y_pred_xgb),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_NOTEBOOK == False and TRAIN_MODE == True:\n",
    "    submission = {\n",
    "        'id':df_test['id'] ,\n",
    "        'Calories': xgb_sub_pred\n",
    "    }\n",
    "\n",
    "    df_sub = pd.DataFrame(submission)\n",
    "    df_sub.to_csv('submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
